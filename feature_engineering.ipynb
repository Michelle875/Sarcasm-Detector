{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a024aa3",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ebb3a8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a9e93b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  ['state', 'slow', 'to', 'shut', 'down', 'weak'...      0\n",
      "1  ['drone', 'place', 'fresh', 'kill', 'on', 'ste...      1\n",
      "2  ['report', ':', 'majority', 'of', 'instance', ...      1\n",
      "3  ['sole', 'remain', 'lung', 'fill', 'with', 'ri...      1\n",
      "4      ['the', 'gop', \"'s\", 'stockholm', 'syndrome']      0\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "\n",
    "df_train = pd.read_csv('train_preprocessed.csv')\n",
    "df_valid = pd.read_csv('valid_preprocessed.csv')\n",
    "df_test = pd.read_csv('test_preprocessed.csv')\n",
    "\n",
    "print(df_train.head())\n",
    "# print(df_valid.head())\n",
    "# print(df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e2a7af",
   "metadata": {},
   "source": [
    "## Tf-idf vectorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "825dcbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 16124)\t0.3992580875734305\n",
      "  (0, 6733)\t0.33285362959347153\n",
      "  (0, 20569)\t0.3175994757862391\n",
      "  (0, 22539)\t0.3959532813475212\n",
      "  (0, 6367)\t0.25803768539922634\n",
      "  (0, 18737)\t0.35965961259181517\n",
      "  (0, 20979)\t0.10798595426714692\n",
      "  (0, 19058)\t0.3992580875734305\n",
      "  (0, 19687)\t0.32426307516649433\n",
      "  (1, 10020)\t0.27323780791536617\n",
      "  (1, 22708)\t0.27847832528799155\n",
      "  (1, 14322)\t0.13429432618359566\n",
      "  (1, 19751)\t0.3914062605065885\n",
      "  (1, 14409)\t0.1757311845631749\n",
      "  (1, 11427)\t0.35063042205931494\n",
      "  (1, 8332)\t0.42908915986669816\n",
      "  (1, 15455)\t0.3966897313978762\n",
      "  (1, 6472)\t0.4257390088404465\n",
      "  (2, 22816)\t0.2929305151697847\n",
      "  (2, 22760)\t0.2598197862928088\n",
      "  (2, 2919)\t0.3136336167987538\n",
      "  (2, 22252)\t0.2392239245628198\n",
      "  (2, 688)\t0.14737869534929005\n",
      "  (2, 10303)\t0.2598197862928088\n",
      "  (2, 14299)\t0.3373086685585056\n",
      "  :\t:\n",
      "  (8, 12252)\t0.2920963651267997\n",
      "  (8, 20928)\t0.20738491930618924\n",
      "  (8, 13479)\t0.20220106107946975\n",
      "  (8, 5220)\t0.3292318271196743\n",
      "  (8, 14339)\t0.28874743883760884\n",
      "  (8, 11832)\t0.28874743883760884\n",
      "  (8, 15007)\t0.38396894691749117\n",
      "  (8, 7472)\t0.37017306196763494\n",
      "  (8, 13935)\t0.15728630934423435\n",
      "  (8, 20979)\t0.0924327384723301\n",
      "  (9, 416)\t0.2812484111635782\n",
      "  (9, 15620)\t0.22641758714793883\n",
      "  (9, 22167)\t0.28213023055119096\n",
      "  (9, 8474)\t0.26068423189281587\n",
      "  (9, 8151)\t0.12114533020889887\n",
      "  (9, 11256)\t0.2726671975189106\n",
      "  (9, 3582)\t0.29341867696078977\n",
      "  (9, 16592)\t0.32997237936466306\n",
      "  (9, 7122)\t0.34902337170903913\n",
      "  (9, 1952)\t0.1759872931005347\n",
      "  (9, 22774)\t0.19142326787293626\n",
      "  (9, 20781)\t0.2329726743876583\n",
      "  (9, 9937)\t0.3331132000920421\n",
      "  (9, 13798)\t0.20539370464624457\n",
      "  (9, 14322)\t0.20303161113987825\n"
     ]
    }
   ],
   "source": [
    "df_train_str = pd.read_csv('train.csv')['text']\n",
    "df_valid_str = pd.read_csv('valid.csv')['text']\n",
    "df_test_str  = pd.read_csv('test.csv')['text']\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(lowercase=False)\n",
    "train_tfidf = vectorizer.fit_transform(df_train_str)\n",
    "valid_tfidf= vectorizer.transform(df_valid_str)\n",
    "test_tfidf  = vectorizer.transform(df_test_str)\n",
    "\n",
    "print(train_tfidf[:10, :]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd21fe63",
   "metadata": {},
   "source": [
    "## Static embeddings - hyperparameter: window size, GloVe vs word2vec\n",
    "word2vec context is interesting, but rare co-occurence can also indicate sarcasm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab09b07e",
   "metadata": {},
   "source": [
    "## Sentiment frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "90b2824a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label  sentiment\n",
      "0  ['state', 'slow', 'to', 'shut', 'down', 'weak'...      0    -0.4404\n",
      "1  ['drone', 'place', 'fresh', 'kill', 'on', 'ste...      1    -0.5267\n",
      "2  ['report', ':', 'majority', 'of', 'instance', ...      1     0.0000\n",
      "3  ['sole', 'remain', 'lung', 'fill', 'with', 'ri...      1     0.7650\n",
      "4      ['the', 'gop', \"'s\", 'stockholm', 'syndrome']      0     0.0000\n",
      "                                                text  label  sentiment\n",
      "0         ['prejudice', 'do', 'not', 'discriminate']      0    -0.5106\n",
      "1  ['entire', 'house', 'implicate', 'by', 'phish'...      1     0.0000\n",
      "2  ['lustful', 'man', 'sensually', 'use', 'one', ...      1     0.4939\n",
      "3  ['area', 'man', 'get', 'terrible', 'creative',...      1    -0.0516\n",
      "4  ['college', 'graduate', 'first', 'person', 'in...      1    -0.4215\n",
      "                                                text  label  sentiment\n",
      "0  ['intuition', 'or', 'ego', '?', '3', 'simple',...      0     0.3400\n",
      "1  ['physicist', 'brings', 'in', 'particle', 'fro...      1     0.0000\n",
      "2  ['donald', 'trump', 'inspire', 'new', 'nsfw', ...      0     0.5719\n",
      "3  ['trump', 'style', ':', 'insult', 'and', 'dome...      0    -0.8176\n",
      "4  ['man', 'google', \"'tender\", 'lump', 'on', 'ne...      1     0.4767\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import ast\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "nltk.download('vader_lexicon', quiet = True)\n",
    "\n",
    "dftrainSent = pd.read_csv(\"train_preprocessed.csv\")\n",
    "dfvalidationSent = pd.read_csv(\"valid_preprocessed.csv\")\n",
    "dftestSent = pd.read_csv(\"test_preprocessed.csv\")\n",
    "\n",
    "\n",
    "# Convert list → sentence\n",
    "dftrainSent['text'] = dftrainSent['text'].apply(lambda x: \" \".join(ast.literal_eval(x)))\n",
    "dfvalidationSent['text'] = dfvalidationSent['text'].apply(lambda x: \" \".join(ast.literal_eval(x)))\n",
    "dftestSent['text'] = dftestSent['text'].apply(lambda x: \" \".join(ast.literal_eval(x)))\n",
    "\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "df_train['sentiment'] = dftrainSent['text'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "df_valid['sentiment'] = dfvalidationSent['text'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "df_test['sentiment'] = dftestSent['text'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "\n",
    "\n",
    "print(df_train.head())\n",
    "print(df_valid.head())\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209ac5a5",
   "metadata": {},
   "source": [
    "## Sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6437ceb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label  sentiment  \\\n",
      "0  ['state', 'slow', 'to', 'shut', 'down', 'weak'...      0    -0.4404   \n",
      "1  ['drone', 'place', 'fresh', 'kill', 'on', 'ste...      1    -0.5267   \n",
      "2  ['report', ':', 'majority', 'of', 'instance', ...      1     0.0000   \n",
      "3  ['sole', 'remain', 'lung', 'fill', 'with', 'ri...      1     0.7650   \n",
      "4      ['the', 'gop', \"'s\", 'stockholm', 'syndrome']      0     0.0000   \n",
      "\n",
      "   length_words  \n",
      "0             9  \n",
      "1             9  \n",
      "2            20  \n",
      "3             9  \n",
      "4             5  \n",
      "                                                text  label  sentiment  \\\n",
      "0         ['prejudice', 'do', 'not', 'discriminate']      0    -0.5106   \n",
      "1  ['entire', 'house', 'implicate', 'by', 'phish'...      1     0.0000   \n",
      "2  ['lustful', 'man', 'sensually', 'use', 'one', ...      1     0.4939   \n",
      "3  ['area', 'man', 'get', 'terrible', 'creative',...      1    -0.0516   \n",
      "4  ['college', 'graduate', 'first', 'person', 'in...      1    -0.4215   \n",
      "\n",
      "   length_words  \n",
      "0             4  \n",
      "1             6  \n",
      "2            12  \n",
      "3             7  \n",
      "4            10  \n",
      "                                                text  label  sentiment  \\\n",
      "0  ['intuition', 'or', 'ego', '?', '3', 'simple',...      0     0.3400   \n",
      "1  ['physicist', 'brings', 'in', 'particle', 'fro...      1     0.0000   \n",
      "2  ['donald', 'trump', 'inspire', 'new', 'nsfw', ...      0     0.5719   \n",
      "3  ['trump', 'style', ':', 'insult', 'and', 'dome...      0    -0.8176   \n",
      "4  ['man', 'google', \"'tender\", 'lump', 'on', 'ne...      1     0.4767   \n",
      "\n",
      "   length_words  \n",
      "0            10  \n",
      "1            12  \n",
      "2            11  \n",
      "3             7  \n",
      "4            15  \n"
     ]
    }
   ],
   "source": [
    "dftrainLen = pd.read_csv(\"train_preprocessed.csv\")\n",
    "dfvalidationLen = pd.read_csv(\"valid_preprocessed.csv\")\n",
    "dftestLen = pd.read_csv(\"test_preprocessed.csv\")\n",
    "\n",
    "df_train['length_words'] = dftrainLen['text'].apply(lambda x: len(x.split()))\n",
    "df_valid['length_words'] = dfvalidationLen['text'].apply(lambda x: len(x.split()))\n",
    "df_test['length_words'] = dftestLen['text'].apply(lambda x: len(x.split()))\n",
    "print(df_train.head())\n",
    "print(df_valid.head())\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ef7130",
   "metadata": {},
   "source": [
    "## Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a31a67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "404f1d0a",
   "metadata": {},
   "source": [
    "## first/last word frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09185307",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05ed2593",
   "metadata": {},
   "source": [
    "## Bag of words (n-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d39fd706",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\acotl\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 153371\n",
      "Some features: ['!' '! !' \"! '\" \"! 'westworld\" '! (' '! )' '! ,' '! ;' '! again' '! and'\n",
      " '! announce' '! british' '! bronco' '! but' '! change' '! check'\n",
      " '! dance' '! die' '! disability' '! dumbledore' '! early' '! episode'\n",
      " '! for' '! hillary' '! how' '! huffpost' '! in' '! kill' '! man' '! my']\n",
      "to 6730\n",
      "of 4679\n",
      "the 3894\n",
      "'s 3839\n",
      "in 3455\n",
      "' 3298\n",
      "be 3147\n",
      ", 2978\n",
      "a 2972\n",
      "for 2701\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# df_train['text'] should be like: [['I', 'love', 'this', '!'], ['So', 'funny', '...'], ...]\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def identity_tokenizer(tokens):\n",
    "    return tokens  \n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    tokenizer=identity_tokenizer,\n",
    "    preprocessor=lambda x: x, \n",
    "    ngram_range=(1,2),\n",
    "    lowercase=False\n",
    ")\n",
    "\n",
    "import ast\n",
    "\n",
    "df_train['text'] = df_train['text'].apply(ast.literal_eval)\n",
    "df_valid['text'] = df_valid['text'].apply(ast.literal_eval)\n",
    "df_test['text']  = df_test['text'].apply(ast.literal_eval)\n",
    "\n",
    "\n",
    "X_train_bow = vectorizer.fit_transform(df_train['text'])\n",
    "X_valid_bow = vectorizer.transform(df_valid['text'])\n",
    "X_test_bow  = vectorizer.transform(df_test['text'])\n",
    "\n",
    "\n",
    "print(\"Number of features:\", len(vectorizer.get_feature_names_out()))\n",
    "print(\"Some features:\", vectorizer.get_feature_names_out()[:30])\n",
    "word_counts = np.asarray(X_train_bow.sum(axis=0)).flatten()\n",
    "\n",
    "features = vectorizer.get_feature_names_out()\n",
    "freq_list = list(zip(features, word_counts))\n",
    "\n",
    "freq_list_sorted = sorted(freq_list, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for word, count in freq_list_sorted[:10]:\n",
    "    print(word, count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfad2b11",
   "metadata": {},
   "source": [
    "## Parts of speech frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "267bb35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\acotl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\acotl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    word pos\n",
      "0  state  NN\n",
      "1   slow  NN\n",
      "2     to  TO\n",
      "3   shut  VB\n",
      "4   down  RP\n",
      "['NN' 'TO' 'VB' 'RP' 'JJ' 'IN' ':' 'NNS' 'VBP' 'RB' 'VBG' ',' 'DT' 'POS'\n",
      " 'CD' 'PRP$' \"''\" 'JJR' 'MD' 'PRP' 'PDT' 'VBN' 'WRB' 'CC' 'VBZ' 'NNP'\n",
      " 'JJS' 'RBS' 'WP' '.' '(' ')' 'SYM' 'VBD' 'RBR' 'WDT' '$' '``' 'UH' '#'\n",
      " 'EX' 'NNPS' 'FW' 'WP$']\n",
      "    word fine_pos high_level_pos\n",
      "0  state       NN              N\n",
      "1   slow       NN              N\n",
      "2     to       TO              P\n",
      "3   shut       VB              V\n",
      "4   down       RP              O\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'): \n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN   # default\n",
    "\n",
    "def tokens_to_pos_df(df, token_col='tokens'):\n",
    "    words = []\n",
    "    pos_tags = []\n",
    "\n",
    "    for tokens in df[token_col]:\n",
    "        tagged = nltk.pos_tag(tokens)   # list of (word, pos)\n",
    "        for word, pos in tagged:\n",
    "            words.append(word)\n",
    "            pos_tags.append(pos)\n",
    "\n",
    "    return pd.DataFrame({'word': words, 'pos': pos_tags})\n",
    "\n",
    "\n",
    "pos_df = tokens_to_pos_df(df_train, 'text')\n",
    "print(pos_df.head())\n",
    "print(pos_df['pos'].unique())\n",
    "\n",
    "# POS meanings\n",
    "'''\n",
    "NN - Noun, singular (dog, car)\n",
    "NNS - Noun, plural (dogs, cars)\n",
    "NNP - Proper noun, singular (London, John)\n",
    "NNPS - Proper noun, plural (Americans)\n",
    "PRP - Personal pronoun (I, you, he)\n",
    "PRP$ - Possessive pronoun (my, your, his)\n",
    "WP - Wh-pronoun (who, what)\n",
    "WP$ - Possessive wh-pronoun (whose)\n",
    "EX - Existential \"there\" (There is…)\n",
    "\n",
    "VB - Verb, base form (eat, run)\n",
    "VBD - Verb, past tense (ate, ran)\n",
    "VBG - Verb, gerund/present participle (eating, running)\n",
    "VBN - Verb, past participle (eaten, run)\n",
    "VBP - Verb, non-3rd person singular present (I eat, they run)\n",
    "VBZ - Verb, 3rd person singular present (she eats)\n",
    "\n",
    "JJ - Adjective\n",
    "JJR - Comparative adjective (bigger)\n",
    "JJS - Superlative adjective (biggest)\n",
    "RB - Adverb (quickly)\n",
    "RBR - Comparative adverb (faster)\n",
    "RBS - Superlative adverb (fastest)\n",
    "WRB - Wh-adverb (where, when, why)\n",
    "\n",
    "IN - Preposition or subordinating conjunction (in, of, because)\n",
    "TO - \"to\"\n",
    "DT - Determiner (the, a)\n",
    "PDT - Predeterminer (all the kids)\n",
    "CC - Coordinating conjunction (and, but)\n",
    "MD - Modal (can, should)\n",
    "\n",
    "RP - Particle (up, off) — as in \"pick up\"\n",
    "POS - Possessive ending ('s)\n",
    "CD - Cardinal number (1, three)\n",
    "UH - Interjection (oh, wow)\n",
    "FW - Foreign word\n",
    "SYM - Symbol\n",
    "$ - Dollar sign\n",
    "# - Pound/hash sign\n",
    "\n",
    ". - Sentence-final punctuation\n",
    ", - Comma\n",
    ": - Colon or semicolon\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# I will also add in a high level POS\n",
    "''' \n",
    "N - \n",
    "NN\n",
    "NNS\n",
    "NNP \n",
    "NNPS \n",
    "PRP\n",
    "PRP$\n",
    "WP \n",
    "WP$\n",
    "EX \n",
    "\n",
    "V-\n",
    "VB \n",
    "VBD \n",
    "VBG \n",
    "VBN\n",
    "VBP\n",
    "VBZ\n",
    "\n",
    "A-\n",
    "JJ\n",
    "JJR \n",
    "JJS\n",
    "RB \n",
    "RBR\n",
    "RBS\n",
    "WRB\n",
    "\n",
    "P-\n",
    "IN \n",
    "TO \n",
    "DT \n",
    "PDT \n",
    "CC \n",
    "MD\n",
    "\n",
    "O:\n",
    "RP \n",
    "POS \n",
    "CD \n",
    "UH\n",
    "FW\n",
    "SYM \n",
    "$\n",
    "# \n",
    "\n",
    "U-\n",
    ".\n",
    ", \n",
    ":\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "pos_mapping = {\n",
    "    # Nouns\n",
    "    'NN': 'N', 'NNS': 'N', 'NNP': 'N', 'NNPS': 'N', \n",
    "    'PRP': 'N', 'PRP$': 'N', 'WP': 'N', 'WP$': 'N', 'EX': 'N',\n",
    "    \n",
    "    # Verbs\n",
    "    'VB': 'V', 'VBD': 'V', 'VBG': 'V', 'VBN': 'V', 'VBP': 'V', 'VBZ': 'V',\n",
    "    \n",
    "    # Adjectives / Adverbs\n",
    "    'JJ': 'A', 'JJR': 'A', 'JJS': 'A', \n",
    "    'RB': 'A', 'RBR': 'A', 'RBS': 'A', 'WRB': 'A',\n",
    "    \n",
    "    # Prepositions / Determiners / Modals / Conjunctions\n",
    "    'IN': 'P', 'TO': 'P', 'DT': 'P', 'PDT': 'P', 'CC': 'P', 'MD': 'P',\n",
    "    \n",
    "    # Other\n",
    "    'RP': 'O', 'POS': 'O', 'CD': 'O', 'UH': 'O', 'FW': 'O', 'SYM': 'O', '$': 'O', '#': 'O',\n",
    "    \n",
    "    # Punctuation\n",
    "    '.': 'U', ',': 'U', ':': 'U'\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "def words_with_pos(df, token_col='text'):\n",
    "    \"\"\"\n",
    "    Takes a DataFrame with tokenized text and returns a DataFrame with:\n",
    "    word | fine_pos | high_level_pos\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for tokens in df[token_col]:\n",
    "        pos_tags = nltk.pos_tag(tokens)\n",
    "        for word, fine_pos in pos_tags:\n",
    "            high_pos = pos_mapping.get(fine_pos, 'O')  # default 'O' if not mapped\n",
    "            rows.append({'word': word, 'fine_pos': fine_pos, 'high_level_pos': high_pos})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Usage\n",
    "df_words = words_with_pos(df_train, token_col='text')\n",
    "print(df_words.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e7364163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label  sentiment  \\\n",
      "0  [state, slow, to, shut, down, weak, teacher, e...      0    -0.4404   \n",
      "1  [drone, place, fresh, kill, on, step, of, whit...      1    -0.5267   \n",
      "2  [report, :, majority, of, instance, of, people...      1     0.0000   \n",
      "3  [sole, remain, lung, fill, with, rich, ,, sati...      1     0.7650   \n",
      "4                [the, gop, 's, stockholm, syndrome]      0     0.0000   \n",
      "\n",
      "   length_words  N  V  A  P  O  U  \n",
      "0             9  4  1  2  1  1  0  \n",
      "1             9  5  0  2  2  0  0  \n",
      "2            20  8  3  3  5  0  1  \n",
      "3             9  5  0  2  1  0  1  \n",
      "4             5  2  0  1  1  1  0  \n",
      "                                                text  label  sentiment  \\\n",
      "0                 [prejudice, do, not, discriminate]      0    -0.5106   \n",
      "1      [entire, house, implicate, by, phish, poster]      1     0.0000   \n",
      "2  [lustful, man, sensually, use, one, hand, to, ...      1     0.4939   \n",
      "3  [area, man, get, terrible, creative, juice, flow]      1    -0.0516   \n",
      "4  [college, graduate, first, person, in, family,...      1    -0.4215   \n",
      "\n",
      "   length_words  N  V  A  P  O  U  \n",
      "0             4  1  2  1  0  0  0  \n",
      "1             6  3  0  2  1  0  0  \n",
      "2            12  5  2  2  2  1  0  \n",
      "3             7  4  1  2  0  0  0  \n",
      "4            10  4  1  1  2  2  0  \n",
      "                                                text  label  sentiment  \\\n",
      "0  [intuition, or, ego, ?, 3, simple, step, to, r...      0     0.3400   \n",
      "1  [physicist, brings, in, particle, from, home, ...      1     0.0000   \n",
      "2  [donald, trump, inspire, new, nsfw, meaning, o...      0     0.5719   \n",
      "3    [trump, style, :, insult, and, domestic, abuse]      0    -0.8176   \n",
      "4  [man, google, 'tender, lump, on, neck, ', abou...      1     0.4767   \n",
      "\n",
      "   length_words  N  V  A  P  O  U  \n",
      "0            10  4  1  1  2  1  1  \n",
      "1            12  4  3  2  3  0  0  \n",
      "2            11  3  0  4  2  2  0  \n",
      "3             7  3  0  2  1  0  1  \n",
      "4            15  6  1  2  4  2  0  \n"
     ]
    }
   ],
   "source": [
    "def pos_counts(tokens):\n",
    "    counts = {'N':0, 'V':0, 'A':0, 'P':0, 'O':0, 'U':0}\n",
    "    for word, fine_pos in nltk.pos_tag(tokens):\n",
    "        high_pos = pos_mapping.get(fine_pos, 'O')  # default 'O' if not mapped\n",
    "        counts[high_pos] += 1\n",
    "    return counts\n",
    "\n",
    "# Ensure all text columns are lists\n",
    "dftrainSent['text'] = dftrainSent['text'].apply(lambda x: x.split() if isinstance(x, str) else x)\n",
    "dfvalidationSent['text'] = dfvalidationSent['text'].apply(lambda x: x.split() if isinstance(x, str) else x)\n",
    "dftestSent['text'] = dftestSent['text'].apply(lambda x: x.split() if isinstance(x, str) else x)\n",
    "\n",
    "# Compute POS counts\n",
    "pos_features_train = dftrainSent['text'].apply(pos_counts).apply(pd.Series)\n",
    "pos_features_validation = dfvalidationSent['text'].apply(pos_counts).apply(pd.Series)\n",
    "pos_features_test = dftestSent['text'].apply(pos_counts).apply(pd.Series)\n",
    "\n",
    "df_train = pd.concat([df_train, pos_features_train], axis=1)\n",
    "df_valid = pd.concat([df_valid, pos_features_validation], axis=1)\n",
    "df_test = pd.concat([df_test, pos_features_test], axis=1)\n",
    "\n",
    "\n",
    "# Check results\n",
    "print(df_train.head())\n",
    "print(df_valid.head())\n",
    "print(df_test.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
