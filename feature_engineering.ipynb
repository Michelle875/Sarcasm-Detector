{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a024aa3",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "ebb3a8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "a9e93b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  ['state', 'slow', 'to', 'shut', 'down', 'weak'...      0\n",
      "1  ['drone', 'place', 'fresh', 'kill', 'on', 'ste...      1\n",
      "2  ['report', ':', 'majority', 'of', 'instance', ...      1\n",
      "3  ['sole', 'remain', 'lung', 'fill', 'with', 'ri...      1\n",
      "4      ['the', 'gop', \"'s\", 'stockholm', 'syndrome']      0\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "\n",
    "df_train = pd.read_csv('train_preprocessed.csv')\n",
    "df_valid = pd.read_csv('valid_preprocessed.csv')\n",
    "df_test = pd.read_csv('test_preprocessed.csv')\n",
    "\n",
    "print(df_train.head())\n",
    "# print(df_valid.head())\n",
    "# print(df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e2a7af",
   "metadata": {},
   "source": [
    "## Tf-idf vectorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "825dcbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 96 stored elements and shape (10, 23222)>\n",
      "  Coords\tValues\n",
      "  (0, 19687)\t0.32426307516649433\n",
      "  (0, 19058)\t0.3992580875734305\n",
      "  (0, 20979)\t0.10798595426714692\n",
      "  (0, 18737)\t0.35965961259181517\n",
      "  (0, 6367)\t0.25803768539922634\n",
      "  (0, 22539)\t0.3959532813475212\n",
      "  (0, 20569)\t0.3175994757862391\n",
      "  (0, 6733)\t0.33285362959347153\n",
      "  (0, 16124)\t0.3992580875734305\n",
      "  (1, 6472)\t0.4257390088404465\n",
      "  (1, 15455)\t0.3966897313978762\n",
      "  (1, 8332)\t0.42908915986669816\n",
      "  (1, 11427)\t0.35063042205931494\n",
      "  (1, 14409)\t0.1757311845631749\n",
      "  (1, 19751)\t0.3914062605065885\n",
      "  (1, 14322)\t0.13429432618359566\n",
      "  (1, 22708)\t0.27847832528799155\n",
      "  (1, 10020)\t0.27323780791536617\n",
      "  (2, 20979)\t0.0770000428999051\n",
      "  (2, 14409)\t0.11562957690780681\n",
      "  (2, 14322)\t0.1767289756377389\n",
      "  (2, 17158)\t0.16578859502292492\n",
      "  (2, 12480)\t0.24121415038283242\n",
      "  (2, 10690)\t0.35115768604059283\n",
      "  (2, 15151)\t0.17922721469222705\n",
      "  :\t:\n",
      "  (8, 7472)\t0.370173061967635\n",
      "  (8, 15007)\t0.3839689469174912\n",
      "  (8, 11832)\t0.2887474388376089\n",
      "  (8, 14339)\t0.2887474388376089\n",
      "  (8, 5220)\t0.32923182711967436\n",
      "  (8, 13479)\t0.20220106107946978\n",
      "  (8, 20928)\t0.20738491930618927\n",
      "  (8, 12252)\t0.29209636512679976\n",
      "  (8, 4177)\t0.3598687795887432\n",
      "  (8, 17261)\t0.32923182711967436\n",
      "  (9, 14322)\t0.20303161113987828\n",
      "  (9, 13798)\t0.2053937046462446\n",
      "  (9, 9937)\t0.3331132000920421\n",
      "  (9, 20781)\t0.23297267438765834\n",
      "  (9, 22774)\t0.1914232678729363\n",
      "  (9, 1952)\t0.17598729310053474\n",
      "  (9, 7122)\t0.3490233717090392\n",
      "  (9, 16592)\t0.3299723793646631\n",
      "  (9, 3582)\t0.2934186769607898\n",
      "  (9, 11256)\t0.27266719751891066\n",
      "  (9, 8151)\t0.1211453302088989\n",
      "  (9, 8474)\t0.26068423189281587\n",
      "  (9, 22167)\t0.28213023055119096\n",
      "  (9, 15620)\t0.22641758714793886\n",
      "  (9, 416)\t0.28124841116357824\n"
     ]
    }
   ],
   "source": [
    "df_train_str = pd.read_csv('train.csv')['text']\n",
    "df_valid_str = pd.read_csv('valid.csv')['text']\n",
    "df_test_str  = pd.read_csv('test.csv')['text']\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(lowercase=False)\n",
    "train_tfidf = vectorizer.fit_transform(df_train_str)\n",
    "valid_tfidf= vectorizer.transform(df_valid_str)\n",
    "test_tfidf  = vectorizer.transform(df_test_str)\n",
    "\n",
    "print(train_tfidf[:10, :]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd21fe63",
   "metadata": {},
   "source": [
    "## Static embeddings - hyperparameter: window size, GloVe vs word2vec\n",
    "word2vec context is interesting, but rare co-occurence can also indicate sarcasm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab09b07e",
   "metadata": {},
   "source": [
    "## Sentiment frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "90b2824a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label  sentiment\n",
      "0  ['state', 'slow', 'to', 'shut', 'down', 'weak'...      0    -0.4404\n",
      "1  ['drone', 'place', 'fresh', 'kill', 'on', 'ste...      1    -0.5267\n",
      "2  ['report', ':', 'majority', 'of', 'instance', ...      1     0.0000\n",
      "3  ['sole', 'remain', 'lung', 'fill', 'with', 'ri...      1     0.7650\n",
      "4      ['the', 'gop', \"'s\", 'stockholm', 'syndrome']      0     0.0000\n",
      "                                                text  label  sentiment\n",
      "0         ['prejudice', 'do', 'not', 'discriminate']      0    -0.5106\n",
      "1  ['entire', 'house', 'implicate', 'by', 'phish'...      1     0.0000\n",
      "2  ['lustful', 'man', 'sensually', 'use', 'one', ...      1     0.4939\n",
      "3  ['area', 'man', 'get', 'terrible', 'creative',...      1    -0.0516\n",
      "4  ['college', 'graduate', 'first', 'person', 'in...      1    -0.4215\n",
      "                                                text  label  sentiment\n",
      "0  ['intuition', 'or', 'ego', '?', '3', 'simple',...      0     0.3400\n",
      "1  ['physicist', 'brings', 'in', 'particle', 'fro...      1     0.0000\n",
      "2  ['donald', 'trump', 'inspire', 'new', 'nsfw', ...      0     0.5719\n",
      "3  ['trump', 'style', ':', 'insult', 'and', 'dome...      0    -0.8176\n",
      "4  ['man', 'google', \"'tender\", 'lump', 'on', 'ne...      1     0.4767\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import ast\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "nltk.download('vader_lexicon', quiet = True)\n",
    "\n",
    "dftrainSent = pd.read_csv(\"train_preprocessed.csv\")\n",
    "dfvalidationSent = pd.read_csv(\"valid_preprocessed.csv\")\n",
    "dftestSent = pd.read_csv(\"test_preprocessed.csv\")\n",
    "\n",
    "\n",
    "# Convert list → sentence\n",
    "dftrainSent['text'] = dftrainSent['text'].apply(lambda x: \" \".join(ast.literal_eval(x)))\n",
    "dfvalidationSent['text'] = dfvalidationSent['text'].apply(lambda x: \" \".join(ast.literal_eval(x)))\n",
    "dftestSent['text'] = dftestSent['text'].apply(lambda x: \" \".join(ast.literal_eval(x)))\n",
    "\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "df_train['sentiment'] = dftrainSent['text'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "df_valid['sentiment'] = dfvalidationSent['text'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "df_test['sentiment'] = dftestSent['text'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "\n",
    "\n",
    "print(df_train.head())\n",
    "print(df_valid.head())\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209ac5a5",
   "metadata": {},
   "source": [
    "## Sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "6437ceb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label  sentiment  \\\n",
      "0  ['state', 'slow', 'to', 'shut', 'down', 'weak'...      0    -0.4404   \n",
      "1  ['drone', 'place', 'fresh', 'kill', 'on', 'ste...      1    -0.5267   \n",
      "2  ['report', ':', 'majority', 'of', 'instance', ...      1     0.0000   \n",
      "3  ['sole', 'remain', 'lung', 'fill', 'with', 'ri...      1     0.7650   \n",
      "4      ['the', 'gop', \"'s\", 'stockholm', 'syndrome']      0     0.0000   \n",
      "\n",
      "   length_words  \n",
      "0             9  \n",
      "1             9  \n",
      "2            20  \n",
      "3             9  \n",
      "4             5  \n",
      "                                                text  label  sentiment  \\\n",
      "0         ['prejudice', 'do', 'not', 'discriminate']      0    -0.5106   \n",
      "1  ['entire', 'house', 'implicate', 'by', 'phish'...      1     0.0000   \n",
      "2  ['lustful', 'man', 'sensually', 'use', 'one', ...      1     0.4939   \n",
      "3  ['area', 'man', 'get', 'terrible', 'creative',...      1    -0.0516   \n",
      "4  ['college', 'graduate', 'first', 'person', 'in...      1    -0.4215   \n",
      "\n",
      "   length_words  \n",
      "0             4  \n",
      "1             6  \n",
      "2            12  \n",
      "3             7  \n",
      "4            10  \n",
      "                                                text  label  sentiment  \\\n",
      "0  ['intuition', 'or', 'ego', '?', '3', 'simple',...      0     0.3400   \n",
      "1  ['physicist', 'brings', 'in', 'particle', 'fro...      1     0.0000   \n",
      "2  ['donald', 'trump', 'inspire', 'new', 'nsfw', ...      0     0.5719   \n",
      "3  ['trump', 'style', ':', 'insult', 'and', 'dome...      0    -0.8176   \n",
      "4  ['man', 'google', \"'tender\", 'lump', 'on', 'ne...      1     0.4767   \n",
      "\n",
      "   length_words  \n",
      "0            10  \n",
      "1            12  \n",
      "2            11  \n",
      "3             7  \n",
      "4            15  \n"
     ]
    }
   ],
   "source": [
    "dftrainLen = pd.read_csv(\"train_preprocessed.csv\")\n",
    "dfvalidationLen = pd.read_csv(\"valid_preprocessed.csv\")\n",
    "dftestLen = pd.read_csv(\"test_preprocessed.csv\")\n",
    "\n",
    "df_train['length_words'] = dftrainLen['text'].apply(lambda x: len(x.split()))\n",
    "df_valid['length_words'] = dfvalidationLen['text'].apply(lambda x: len(x.split()))\n",
    "df_test['length_words'] = dftestLen['text'].apply(lambda x: len(x.split()))\n",
    "print(df_train.head())\n",
    "print(df_valid.head())\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ef7130",
   "metadata": {},
   "source": [
    "## Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "54a31a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation features shape: (21464, 14)\n",
      "\n",
      "Sample punctuation features:\n",
      "   exclamation_count  question_count  ellipsis_count  quote_count  \\\n",
      "0                  0               0               0            0   \n",
      "1                  0               0               0            0   \n",
      "2                  0               0               0            0   \n",
      "3                  0               0               0            0   \n",
      "4                  0               0               0            1   \n",
      "\n",
      "   comma_count  period_count  semicolon_count  colon_count  dash_count  \\\n",
      "0            0             0                0            0           0   \n",
      "1            0             0                0            0           0   \n",
      "2            0             0                0            1           0   \n",
      "3            1             0                0            0           0   \n",
      "4            0             0                0            0           0   \n",
      "\n",
      "   multiple_exclamation  multiple_question  mixed_punctuation  punct_density  \\\n",
      "0                     0                  0                  0          0.000   \n",
      "1                     0                  0                  0          0.000   \n",
      "2                     0                  0                  0          0.000   \n",
      "3                     0                  0                  0          0.125   \n",
      "4                     0                  0                  0          0.000   \n",
      "\n",
      "   all_caps_words  \n",
      "0               0  \n",
      "1               0  \n",
      "2               0  \n",
      "3               0  \n",
      "4               0  \n"
     ]
    }
   ],
   "source": [
    "df_train_str = pd.read_csv('train.csv')['text']\n",
    "df_valid_str = pd.read_csv('valid.csv')['text']\n",
    "df_test_str  = pd.read_csv('test.csv')['text']\n",
    "\n",
    "def extract_punctuation_features(text):\n",
    "    features = {}\n",
    "    features['exclamation_count'] = text.count('!')\n",
    "    features['question_count'] = text.count('?')\n",
    "    features['ellipsis_count'] = len(re.findall(r'\\.{2,}', text))  # Two or more dots\n",
    "    features['quote_count'] = text.count('\"') + text.count(\"'\")\n",
    "    features['comma_count'] = text.count(',')\n",
    "    features['period_count'] = text.count('.')\n",
    "    features['semicolon_count'] = text.count(';')\n",
    "    features['colon_count'] = text.count(':')\n",
    "    features['dash_count'] = text.count('-') + text.count('—')\n",
    "    features['multiple_exclamation'] = len(re.findall(r'!{2,}', text))\n",
    "    features['multiple_question'] = len(re.findall(r'\\?{2,}', text))\n",
    "    features['mixed_punctuation'] = len(re.findall(r'[!?]{2,}', text))\n",
    "    total_punct = sum([features['exclamation_count'], features['question_count'],\n",
    "                       features['comma_count'], features['period_count']])\n",
    "    text_length = len(text.split())\n",
    "    features['punct_density'] = total_punct / max(text_length, 1)\n",
    "    features['all_caps_words'] = len(re.findall(r'\\b[A-Z]{2,}\\b', text))\n",
    "    \n",
    "    return features\n",
    "\n",
    "def create_punctuation_features(text_series):\n",
    "    punct_features = text_series.apply(extract_punctuation_features) \n",
    "    return pd.DataFrame(punct_features.tolist())\n",
    "\n",
    "train_punct = create_punctuation_features(df_train_str)\n",
    "valid_punct = create_punctuation_features(df_valid_str)\n",
    "test_punct = create_punctuation_features(df_test_str)\n",
    "\n",
    "print(\"Punctuation features shape:\", train_punct.shape)\n",
    "print(\"\\nSample punctuation features:\")\n",
    "print(train_punct.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404f1d0a",
   "metadata": {},
   "source": [
    "## first/last word frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "09185307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting first/last words...\n",
      "\n",
      "First word vocabulary size: 200\n",
      "Last word vocabulary size: 200\n",
      "\n",
      "Most common first words: ['the', 'man', 'new', 'report', 'trump', 'how', 'area', 'this', 'why', 'a', 'woman', 'what', 'nation', 'obama', 'donald', 'study', 'watch', 'here', 'u.s.', '5']\n",
      "\n",
      "Most common last words: [\"'\", '?', '.', 'day', 'year', ')', 'time', 'it', 'life', 'say', 'trump', '!', 'woman', 'him', 'again', 'now', 'week', 'you', 'child', 'report']\n",
      "Position features shape: (21464, 400)\n",
      "\n",
      "Sample position features:\n",
      "   first_the  first_man  first_new  first_report  first_trump  first_how  \\\n",
      "0          0          0          0             0            0          0   \n",
      "1          0          0          0             0            0          0   \n",
      "2          0          0          0             1            0          0   \n",
      "3          0          0          0             0            0          0   \n",
      "4          1          0          0             0            0          0   \n",
      "\n",
      "   first_area  first_this  first_why  first_a  ...  last_medium  last_cop  \\\n",
      "0           0           0          0        0  ...            0         0   \n",
      "1           0           0          0        0  ...            0         0   \n",
      "2           0           0          0        0  ...            0         0   \n",
      "3           0           0          0        0  ...            0         0   \n",
      "4           0           0          0        0  ...            0         0   \n",
      "\n",
      "   last_sex  last_record  last_like  last_wall  last_daughter  last_idea  \\\n",
      "0         0            0          0          0              0          0   \n",
      "1         0            0          0          0              0          0   \n",
      "2         0            0          0          0              0          0   \n",
      "3         0            0          0          0              0          0   \n",
      "4         0            0          0          0              0          0   \n",
      "\n",
      "   last_for  last_prison  \n",
      "0         0            0  \n",
      "1         0            0  \n",
      "2         0            0  \n",
      "3         0            0  \n",
      "4         0            0  \n",
      "\n",
      "[5 rows x 400 columns]\n"
     ]
    }
   ],
   "source": [
    "# df_train = pd.read_csv('train_preprocessed.csv')\n",
    "# df_valid = pd.read_csv('valid_preprocessed.csv')\n",
    "# df_test = pd.read_csv('test_preprocessed.csv')\n",
    "def parse_token_list(text_str):\n",
    "    try:\n",
    "        return ast.literal_eval(text_str)\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "def get_first_last_words(df):\n",
    "    tokens = df['text'].apply(parse_token_list)\n",
    "    \n",
    "    first_words = tokens.apply(lambda x: x[0] if len(x) > 0 else '<EMPTY>')\n",
    "    last_words = tokens.apply(lambda x: x[-1] if len(x) > 0 else '<EMPTY>')\n",
    "    \n",
    "    return first_words, last_words\n",
    "\n",
    "print(\"Extracting first/last words...\")\n",
    "train_first, train_last = get_first_last_words(df_train)\n",
    "valid_first, valid_last = get_first_last_words(df_valid)\n",
    "test_first, test_last = get_first_last_words(df_test)\n",
    "\n",
    "\n",
    "def build_position_vocabulary(words_series, top_n=200, min_freq=5):\n",
    "    \n",
    "    word_counts = Counter(words_series)\n",
    "    \n",
    "    filtered_words = {word: count for word, count in word_counts.items() \n",
    "                     if count >= min_freq}\n",
    "    \n",
    "    vocab = [word for word, _ in sorted(filtered_words.items(), \n",
    "                                       key=lambda x: x[1], \n",
    "                                       reverse=True)[:top_n]]\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "first_word_vocab = build_position_vocabulary(train_first, top_n=200, min_freq=5)\n",
    "last_word_vocab = build_position_vocabulary(train_last, top_n=200, min_freq=5)\n",
    "\n",
    "print(f\"\\nFirst word vocabulary size: {len(first_word_vocab)}\")\n",
    "print(f\"Last word vocabulary size: {len(last_word_vocab)}\")\n",
    "\n",
    "print(\"\\nMost common first words:\", first_word_vocab[:20])\n",
    "print(\"\\nMost common last words:\", last_word_vocab[:20])\n",
    "\n",
    "def create_position_features(first_words, last_words, first_vocab, last_vocab):\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    for word in first_vocab:\n",
    "        features[f'first_{word}'] = (first_words == word).astype(int)\n",
    "    \n",
    "    for word in last_vocab:\n",
    "        features[f'last_{word}'] = (last_words == word).astype(int)\n",
    "    \n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "train_position = create_position_features(train_first, train_last, \n",
    "                                         first_word_vocab, last_word_vocab)\n",
    "valid_position = create_position_features(valid_first, valid_last,\n",
    "                                         first_word_vocab, last_word_vocab)\n",
    "test_position = create_position_features(test_first, test_last,\n",
    "                                        first_word_vocab, last_word_vocab)\n",
    "\n",
    "print(\"Position features shape:\", train_position.shape)\n",
    "print(\"\\nSample position features:\")\n",
    "print(train_position.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ed2593",
   "metadata": {},
   "source": [
    "## Bag of words (n-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "d39fd706",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 153371\n",
      "Some features: ['!' '! !' \"! '\" \"! 'westworld\" '! (' '! )' '! ,' '! ;' '! again' '! and'\n",
      " '! announce' '! british' '! bronco' '! but' '! change' '! check'\n",
      " '! dance' '! die' '! disability' '! dumbledore' '! early' '! episode'\n",
      " '! for' '! hillary' '! how' '! huffpost' '! in' '! kill' '! man' '! my']\n",
      "to 6730\n",
      "of 4679\n",
      "the 3894\n",
      "'s 3839\n",
      "in 3455\n",
      "' 3298\n",
      "be 3147\n",
      ", 2978\n",
      "a 2972\n",
      "for 2701\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# df_train['text'] should be like: [['I', 'love', 'this', '!'], ['So', 'funny', '...'], ...]\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def identity_tokenizer(tokens):\n",
    "    return tokens  \n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    tokenizer=identity_tokenizer,\n",
    "    preprocessor=lambda x: x, \n",
    "    ngram_range=(1,2),\n",
    "    lowercase=False\n",
    ")\n",
    "\n",
    "import ast\n",
    "\n",
    "df_train['text'] = df_train['text'].apply(ast.literal_eval)\n",
    "df_valid['text'] = df_valid['text'].apply(ast.literal_eval)\n",
    "df_test['text']  = df_test['text'].apply(ast.literal_eval)\n",
    "\n",
    "\n",
    "X_train_bow = vectorizer.fit_transform(df_train['text'])\n",
    "X_valid_bow = vectorizer.transform(df_valid['text'])\n",
    "X_test_bow  = vectorizer.transform(df_test['text'])\n",
    "\n",
    "\n",
    "print(\"Number of features:\", len(vectorizer.get_feature_names_out()))\n",
    "print(\"Some features:\", vectorizer.get_feature_names_out()[:30])\n",
    "word_counts = np.asarray(X_train_bow.sum(axis=0)).flatten()\n",
    "\n",
    "features = vectorizer.get_feature_names_out()\n",
    "freq_list = list(zip(features, word_counts))\n",
    "\n",
    "freq_list_sorted = sorted(freq_list, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for word, count in freq_list_sorted[:10]:\n",
    "    print(word, count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfad2b11",
   "metadata": {},
   "source": [
    "## Parts of speech frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "267bb35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/michelleshlivko/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/michelleshlivko/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    word pos\n",
      "0  state  NN\n",
      "1   slow  NN\n",
      "2     to  TO\n",
      "3   shut  VB\n",
      "4   down  RP\n",
      "['NN' 'TO' 'VB' 'RP' 'JJ' 'IN' ':' 'NNS' 'VBP' 'RB' 'VBG' ',' 'DT' 'POS'\n",
      " 'CD' 'PRP$' \"''\" 'JJR' 'MD' 'PRP' 'PDT' 'VBN' 'WRB' 'CC' 'VBZ' 'NNP'\n",
      " 'JJS' 'RBS' 'WP' '.' '(' ')' 'SYM' 'VBD' 'RBR' 'WDT' '$' '``' 'UH' '#'\n",
      " 'EX' 'NNPS' 'FW' 'WP$']\n",
      "    word fine_pos high_level_pos\n",
      "0  state       NN              N\n",
      "1   slow       NN              N\n",
      "2     to       TO              P\n",
      "3   shut       VB              V\n",
      "4   down       RP              O\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'): \n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN   # default\n",
    "\n",
    "def tokens_to_pos_df(df, token_col='tokens'):\n",
    "    words = []\n",
    "    pos_tags = []\n",
    "\n",
    "    for tokens in df[token_col]:\n",
    "        tagged = nltk.pos_tag(tokens)   # list of (word, pos)\n",
    "        for word, pos in tagged:\n",
    "            words.append(word)\n",
    "            pos_tags.append(pos)\n",
    "\n",
    "    return pd.DataFrame({'word': words, 'pos': pos_tags})\n",
    "\n",
    "\n",
    "pos_df = tokens_to_pos_df(df_train, 'text')\n",
    "print(pos_df.head())\n",
    "print(pos_df['pos'].unique())\n",
    "\n",
    "# POS meanings\n",
    "'''\n",
    "NN - Noun, singular (dog, car)\n",
    "NNS - Noun, plural (dogs, cars)\n",
    "NNP - Proper noun, singular (London, John)\n",
    "NNPS - Proper noun, plural (Americans)\n",
    "PRP - Personal pronoun (I, you, he)\n",
    "PRP$ - Possessive pronoun (my, your, his)\n",
    "WP - Wh-pronoun (who, what)\n",
    "WP$ - Possessive wh-pronoun (whose)\n",
    "EX - Existential \"there\" (There is…)\n",
    "\n",
    "VB - Verb, base form (eat, run)\n",
    "VBD - Verb, past tense (ate, ran)\n",
    "VBG - Verb, gerund/present participle (eating, running)\n",
    "VBN - Verb, past participle (eaten, run)\n",
    "VBP - Verb, non-3rd person singular present (I eat, they run)\n",
    "VBZ - Verb, 3rd person singular present (she eats)\n",
    "\n",
    "JJ - Adjective\n",
    "JJR - Comparative adjective (bigger)\n",
    "JJS - Superlative adjective (biggest)\n",
    "RB - Adverb (quickly)\n",
    "RBR - Comparative adverb (faster)\n",
    "RBS - Superlative adverb (fastest)\n",
    "WRB - Wh-adverb (where, when, why)\n",
    "\n",
    "IN - Preposition or subordinating conjunction (in, of, because)\n",
    "TO - \"to\"\n",
    "DT - Determiner (the, a)\n",
    "PDT - Predeterminer (all the kids)\n",
    "CC - Coordinating conjunction (and, but)\n",
    "MD - Modal (can, should)\n",
    "\n",
    "RP - Particle (up, off) — as in \"pick up\"\n",
    "POS - Possessive ending ('s)\n",
    "CD - Cardinal number (1, three)\n",
    "UH - Interjection (oh, wow)\n",
    "FW - Foreign word\n",
    "SYM - Symbol\n",
    "$ - Dollar sign\n",
    "# - Pound/hash sign\n",
    "\n",
    ". - Sentence-final punctuation\n",
    ", - Comma\n",
    ": - Colon or semicolon\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# I will also add in a high level POS\n",
    "''' \n",
    "N - \n",
    "NN\n",
    "NNS\n",
    "NNP \n",
    "NNPS \n",
    "PRP\n",
    "PRP$\n",
    "WP \n",
    "WP$\n",
    "EX \n",
    "\n",
    "V-\n",
    "VB \n",
    "VBD \n",
    "VBG \n",
    "VBN\n",
    "VBP\n",
    "VBZ\n",
    "\n",
    "A-\n",
    "JJ\n",
    "JJR \n",
    "JJS\n",
    "RB \n",
    "RBR\n",
    "RBS\n",
    "WRB\n",
    "\n",
    "P-\n",
    "IN \n",
    "TO \n",
    "DT \n",
    "PDT \n",
    "CC \n",
    "MD\n",
    "\n",
    "O:\n",
    "RP \n",
    "POS \n",
    "CD \n",
    "UH\n",
    "FW\n",
    "SYM \n",
    "$\n",
    "# \n",
    "\n",
    "U-\n",
    ".\n",
    ", \n",
    ":\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "pos_mapping = {\n",
    "    # Nouns\n",
    "    'NN': 'N', 'NNS': 'N', 'NNP': 'N', 'NNPS': 'N', \n",
    "    'PRP': 'N', 'PRP$': 'N', 'WP': 'N', 'WP$': 'N', 'EX': 'N',\n",
    "    \n",
    "    # Verbs\n",
    "    'VB': 'V', 'VBD': 'V', 'VBG': 'V', 'VBN': 'V', 'VBP': 'V', 'VBZ': 'V',\n",
    "    \n",
    "    # Adjectives / Adverbs\n",
    "    'JJ': 'A', 'JJR': 'A', 'JJS': 'A', \n",
    "    'RB': 'A', 'RBR': 'A', 'RBS': 'A', 'WRB': 'A',\n",
    "    \n",
    "    # Prepositions / Determiners / Modals / Conjunctions\n",
    "    'IN': 'P', 'TO': 'P', 'DT': 'P', 'PDT': 'P', 'CC': 'P', 'MD': 'P',\n",
    "    \n",
    "    # Other\n",
    "    'RP': 'O', 'POS': 'O', 'CD': 'O', 'UH': 'O', 'FW': 'O', 'SYM': 'O', '$': 'O', '#': 'O',\n",
    "    \n",
    "    # Punctuation\n",
    "    '.': 'U', ',': 'U', ':': 'U'\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "def words_with_pos(df, token_col='text'):\n",
    "    \"\"\"\n",
    "    Takes a DataFrame with tokenized text and returns a DataFrame with:\n",
    "    word | fine_pos | high_level_pos\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for tokens in df[token_col]:\n",
    "        pos_tags = nltk.pos_tag(tokens)\n",
    "        for word, fine_pos in pos_tags:\n",
    "            high_pos = pos_mapping.get(fine_pos, 'O')  # default 'O' if not mapped\n",
    "            rows.append({'word': word, 'fine_pos': fine_pos, 'high_level_pos': high_pos})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Usage\n",
    "df_words = words_with_pos(df_train, token_col='text')\n",
    "print(df_words.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "edf1eb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/michelleshlivko/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('state', 'N'), ('slow', 'N'), ('to', 'P'), ('shut', 'V'), ('down', 'A'), ('weak', 'A'), ('teacher', 'A'), ('education', 'N'), ('program', 'N')]\n",
      "[('drone', 'N'), ('place', 'N'), ('fresh', 'A'), ('kill', 'N'), ('on', 'P'), ('step', 'N'), ('of', 'P'), ('white', 'A'), ('house', 'N')]\n",
      "[('report', 'N'), (':', 'U'), ('majority', 'N'), ('of', 'P'), ('instance', 'N'), ('of', 'P'), ('people', 'N'), ('get', 'V'), ('life', 'N'), ('back', 'A'), ('on', 'P'), ('track', 'N'), ('occur', 'N'), ('immediately', 'A'), ('after', 'P'), ('visit', 'N'), ('to', 'P'), ('buffalo', 'V'), ('wild', 'A'), ('wing', 'V')]\n",
      "[('sole', 'N'), ('remain', 'N'), ('lung', 'N'), ('fill', 'N'), ('with', 'P'), ('rich', 'A'), (',', 'U'), ('satisfy', 'A'), ('flavor', 'N')]\n",
      "[('the', 'P'), ('gop', 'N'), (\"'s\", 'O'), ('stockholm', 'A'), ('syndrome', 'N')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import ast\n",
    "from nltk import pos_tag\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "\n",
    "\n",
    "# Convert string representation of list into actual list\n",
    "# df_train['tokens'] = df_train['text'].apply(ast.literal_eval)\n",
    "\n",
    "# Function to convert POS tags to high-level POS\n",
    "def get_high_level_pos(tag):\n",
    "    if tag.startswith('N'):\n",
    "        return 'N'\n",
    "    elif tag.startswith('V'):\n",
    "        return 'V'\n",
    "    elif tag.startswith('J') or tag.startswith('R'):\n",
    "        return 'A'\n",
    "    elif tag in ['IN','TO','DT','PDT','CC','MD']:\n",
    "        return 'P'\n",
    "    elif tag in ['RP','POS','CD','UH','FW','SYM','$','#']:\n",
    "        return 'O'\n",
    "    elif tag in ['.',';',',',':']:\n",
    "        return 'U'\n",
    "    else:\n",
    "        return 'O'\n",
    "\n",
    "# Tag tokens in context and create new column\n",
    "def tag_tokens(tokens):\n",
    "    tagged = pos_tag(tokens)  # [('word','POS'), ...]\n",
    "    high_level = [get_high_level_pos(tag) for _, tag in tagged]\n",
    "    return high_level\n",
    "\n",
    "df_train['pos_seq'] = df_train['text'].apply(tag_tokens)\n",
    "\n",
    "count = 0\n",
    "for tokens, pos_seq in zip(df_train['text'], df_train['pos_seq']):\n",
    "    if count < 5:\n",
    "        print(list(zip(tokens, pos_seq)))\n",
    "        count += 1\n",
    "    else:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "e7364163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label  sentiment  \\\n",
      "0  [state, slow, to, shut, down, weak, teacher, e...      0    -0.4404   \n",
      "1  [drone, place, fresh, kill, on, step, of, whit...      1    -0.5267   \n",
      "2  [report, :, majority, of, instance, of, people...      1     0.0000   \n",
      "3  [sole, remain, lung, fill, with, rich, ,, sati...      1     0.7650   \n",
      "4                [the, gop, 's, stockholm, syndrome]      0     0.0000   \n",
      "\n",
      "   length_words                                            pos_seq  N  V  A  \\\n",
      "0             9                        [N, N, P, V, A, A, A, N, N]  4  1  2   \n",
      "1             9                        [N, N, A, N, P, N, P, A, N]  5  0  2   \n",
      "2            20  [N, U, N, P, N, P, N, V, N, A, P, N, N, A, P, ...  8  3  3   \n",
      "3             9                        [N, N, N, N, P, A, U, A, N]  5  0  2   \n",
      "4             5                                    [P, N, O, A, N]  2  0  1   \n",
      "\n",
      "   P  O  U  \n",
      "0  1  1  0  \n",
      "1  2  0  0  \n",
      "2  5  0  1  \n",
      "3  1  0  1  \n",
      "4  1  1  0  \n",
      "                                                text  label  sentiment  \\\n",
      "0                 [prejudice, do, not, discriminate]      0    -0.5106   \n",
      "1      [entire, house, implicate, by, phish, poster]      1     0.0000   \n",
      "2  [lustful, man, sensually, use, one, hand, to, ...      1     0.4939   \n",
      "3  [area, man, get, terrible, creative, juice, flow]      1    -0.0516   \n",
      "4  [college, graduate, first, person, in, family,...      1    -0.4215   \n",
      "\n",
      "   length_words  N  V  A  P  O  U  \n",
      "0             4  1  2  1  0  0  0  \n",
      "1             6  3  0  2  1  0  0  \n",
      "2            12  5  2  2  2  1  0  \n",
      "3             7  4  1  2  0  0  0  \n",
      "4            10  4  1  1  2  2  0  \n",
      "                                                text  label  sentiment  \\\n",
      "0  [intuition, or, ego, ?, 3, simple, step, to, r...      0     0.3400   \n",
      "1  [physicist, brings, in, particle, from, home, ...      1     0.0000   \n",
      "2  [donald, trump, inspire, new, nsfw, meaning, o...      0     0.5719   \n",
      "3    [trump, style, :, insult, and, domestic, abuse]      0    -0.8176   \n",
      "4  [man, google, 'tender, lump, on, neck, ', abou...      1     0.4767   \n",
      "\n",
      "   length_words  N  V  A  P  O  U  \n",
      "0            10  4  1  1  2  1  1  \n",
      "1            12  4  3  2  3  0  0  \n",
      "2            11  3  0  4  2  2  0  \n",
      "3             7  3  0  2  1  0  1  \n",
      "4            15  6  1  2  4  2  0  \n"
     ]
    }
   ],
   "source": [
    "def pos_counts(tokens):\n",
    "    counts = {'N':0, 'V':0, 'A':0, 'P':0, 'O':0, 'U':0}\n",
    "    for word, fine_pos in nltk.pos_tag(tokens):\n",
    "        high_pos = pos_mapping.get(fine_pos, 'O')  # default 'O' if not mapped\n",
    "        counts[high_pos] += 1\n",
    "    return counts\n",
    "\n",
    "# Ensure all text columns are lists\n",
    "dftrainSent['text'] = dftrainSent['text'].apply(lambda x: x.split() if isinstance(x, str) else x)\n",
    "dfvalidationSent['text'] = dfvalidationSent['text'].apply(lambda x: x.split() if isinstance(x, str) else x)\n",
    "dftestSent['text'] = dftestSent['text'].apply(lambda x: x.split() if isinstance(x, str) else x)\n",
    "\n",
    "\n",
    "# Compute POS counts\n",
    "pos_features_train = dftrainSent['text'].apply(pos_counts).apply(pd.Series)\n",
    "pos_features_validation = dfvalidationSent['text'].apply(pos_counts).apply(pd.Series)\n",
    "pos_features_test = dftestSent['text'].apply(pos_counts).apply(pd.Series)\n",
    "\n",
    "df_train = pd.concat([df_train, pos_features_train], axis=1)\n",
    "df_valid = pd.concat([df_valid, pos_features_validation], axis=1)\n",
    "df_test = pd.concat([df_test, pos_features_test], axis=1)\n",
    "\n",
    "\n",
    "# Check results\n",
    "print(df_train.head())\n",
    "print(df_valid.head())\n",
    "print(df_test.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "edc6bec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('train_features.csv', index=False)\n",
    "df_valid.to_csv('valid_features.csv', index=False)\n",
    "df_test.to_csv('test_features.csv', index=False)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machineLearning",
   "language": "python",
   "name": "machinelearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
