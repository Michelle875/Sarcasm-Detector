{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a024aa3",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebb3a8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9e93b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  ['state', 'slow', 'to', 'shut', 'down', 'weak'...      0\n",
      "1  ['drone', 'place', 'fresh', 'kill', 'on', 'ste...      1\n",
      "2  ['report', ':', 'majority', 'of', 'instance', ...      1\n",
      "3  ['sole', 'remain', 'lung', 'fill', 'with', 'ri...      1\n",
      "4      ['the', 'gop', \"'s\", 'stockholm', 'syndrome']      0\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "\n",
    "df_train = pd.read_csv('train_preprocessed.csv')\n",
    "df_valid = pd.read_csv('valid_preprocessed.csv')\n",
    "df_test = pd.read_csv('test_preprocessed.csv')\n",
    "\n",
    "print(df_train.head())\n",
    "# print(df_valid.head())\n",
    "# print(df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e2a7af",
   "metadata": {},
   "source": [
    "## Tf-idf vectorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "825dcbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 16124)\t0.3992580875734305\n",
      "  (0, 6733)\t0.33285362959347153\n",
      "  (0, 20569)\t0.3175994757862391\n",
      "  (0, 22539)\t0.3959532813475212\n",
      "  (0, 6367)\t0.25803768539922634\n",
      "  (0, 18737)\t0.35965961259181517\n",
      "  (0, 20979)\t0.10798595426714692\n",
      "  (0, 19058)\t0.3992580875734305\n",
      "  (0, 19687)\t0.32426307516649433\n",
      "  (1, 10020)\t0.27323780791536617\n",
      "  (1, 22708)\t0.27847832528799155\n",
      "  (1, 14322)\t0.13429432618359566\n",
      "  (1, 19751)\t0.3914062605065885\n",
      "  (1, 14409)\t0.1757311845631749\n",
      "  (1, 11427)\t0.35063042205931494\n",
      "  (1, 8332)\t0.42908915986669816\n",
      "  (1, 15455)\t0.3966897313978762\n",
      "  (1, 6472)\t0.4257390088404465\n",
      "  (2, 22816)\t0.2929305151697847\n",
      "  (2, 22760)\t0.2598197862928088\n",
      "  (2, 2919)\t0.3136336167987538\n",
      "  (2, 22252)\t0.2392239245628198\n",
      "  (2, 688)\t0.14737869534929005\n",
      "  (2, 10303)\t0.2598197862928088\n",
      "  (2, 14299)\t0.3373086685585056\n",
      "  :\t:\n",
      "  (8, 12252)\t0.2920963651267997\n",
      "  (8, 20928)\t0.20738491930618924\n",
      "  (8, 13479)\t0.20220106107946975\n",
      "  (8, 5220)\t0.3292318271196743\n",
      "  (8, 14339)\t0.28874743883760884\n",
      "  (8, 11832)\t0.28874743883760884\n",
      "  (8, 15007)\t0.38396894691749117\n",
      "  (8, 7472)\t0.37017306196763494\n",
      "  (8, 13935)\t0.15728630934423435\n",
      "  (8, 20979)\t0.0924327384723301\n",
      "  (9, 416)\t0.2812484111635782\n",
      "  (9, 15620)\t0.22641758714793883\n",
      "  (9, 22167)\t0.28213023055119096\n",
      "  (9, 8474)\t0.26068423189281587\n",
      "  (9, 8151)\t0.12114533020889887\n",
      "  (9, 11256)\t0.2726671975189106\n",
      "  (9, 3582)\t0.29341867696078977\n",
      "  (9, 16592)\t0.32997237936466306\n",
      "  (9, 7122)\t0.34902337170903913\n",
      "  (9, 1952)\t0.1759872931005347\n",
      "  (9, 22774)\t0.19142326787293626\n",
      "  (9, 20781)\t0.2329726743876583\n",
      "  (9, 9937)\t0.3331132000920421\n",
      "  (9, 13798)\t0.20539370464624457\n",
      "  (9, 14322)\t0.20303161113987825\n"
     ]
    }
   ],
   "source": [
    "df_train_str = pd.read_csv('train.csv')['text']\n",
    "df_valid_str = pd.read_csv('valid.csv')['text']\n",
    "df_test_str  = pd.read_csv('test.csv')['text']\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(lowercase=False)\n",
    "train_tfidf = vectorizer.fit_transform(df_train_str)\n",
    "valid_tfidf= vectorizer.transform(df_valid_str)\n",
    "test_tfidf  = vectorizer.transform(df_test_str)\n",
    "\n",
    "print(train_tfidf[:10, :]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd21fe63",
   "metadata": {},
   "source": [
    "## Static embeddings - hyperparameter: window size, GloVe vs word2vec\n",
    "word2vec context is interesting, but rare co-occurence can also indicate sarcasm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab09b07e",
   "metadata": {},
   "source": [
    "## Sentiment frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90b2824a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label  sentiment\n",
      "0  state slow to shut down weak teacher education...      0    -0.4404\n",
      "1      drone place fresh kill on step of white house      1    -0.5267\n",
      "2  report : majority of instance of people get li...      1     0.0000\n",
      "3   sole remain lung fill with rich , satisfy flavor      1     0.7650\n",
      "4                      the gop 's stockholm syndrome      0     0.0000\n",
      "                                                text  label  sentiment\n",
      "0                      prejudice do not discriminate      0    -0.5106\n",
      "1             entire house implicate by phish poster      1     0.0000\n",
      "2  lustful man sensually use one hand to unhook c...      1     0.4939\n",
      "3          area man get terrible creative juice flow      1    -0.0516\n",
      "4  college graduate first person in family to was...      1    -0.4215\n",
      "                                                text  label  sentiment\n",
      "0    intuition or ego ? 3 simple step to reach truth      0     0.3400\n",
      "1  physicist brings in particle from home he 's b...      1     0.0000\n",
      "2  donald trump inspire new nsfw meaning of the a...      0     0.5719\n",
      "3            trump style : insult and domestic abuse      0    -0.8176\n",
      "4  man google 'tender lump on neck ' about to beg...      1     0.4767\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import ast\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "nltk.download('vader_lexicon', quiet = True)\n",
    "\n",
    "dftrainSent = pd.read_csv(\"train_preprocessed.csv\")\n",
    "dfvalidationSent = pd.read_csv(\"valid_preprocessed.csv\")\n",
    "dftestSent = pd.read_csv(\"test_preprocessed.csv\")\n",
    "\n",
    "\n",
    "# Convert list â†’ sentence\n",
    "dftrainSent['text'] = dftrainSent['text'].apply(lambda x: \" \".join(ast.literal_eval(x)))\n",
    "dfvalidationSent['text'] = dfvalidationSent['text'].apply(lambda x: \" \".join(ast.literal_eval(x)))\n",
    "dftestSent['text'] = dftestSent['text'].apply(lambda x: \" \".join(ast.literal_eval(x)))\n",
    "\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "dftrainSent['sentiment'] = dftrainSent['text'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "dfvalidationSent['sentiment'] = dfvalidationSent['text'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "dftestSent['sentiment'] = dftestSent['text'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "\n",
    "\n",
    "print(dftrainSent.head())\n",
    "print(dfvalidationSent.head())\n",
    "print(dftestSent.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209ac5a5",
   "metadata": {},
   "source": [
    "## Sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6437ceb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label  sentiment  \\\n",
      "0  state slow to shut down weak teacher education...      0    -0.4404   \n",
      "1      drone place fresh kill on step of white house      1    -0.5267   \n",
      "2  report : majority of instance of people get li...      1     0.0000   \n",
      "3   sole remain lung fill with rich , satisfy flavor      1     0.7650   \n",
      "4                      the gop 's stockholm syndrome      0     0.0000   \n",
      "\n",
      "   length_words  \n",
      "0             9  \n",
      "1             9  \n",
      "2            20  \n",
      "3             9  \n",
      "4             5  \n",
      "                                                text  label  sentiment  \\\n",
      "0                      prejudice do not discriminate      0    -0.5106   \n",
      "1             entire house implicate by phish poster      1     0.0000   \n",
      "2  lustful man sensually use one hand to unhook c...      1     0.4939   \n",
      "3          area man get terrible creative juice flow      1    -0.0516   \n",
      "4  college graduate first person in family to was...      1    -0.4215   \n",
      "\n",
      "   length_words  \n",
      "0             4  \n",
      "1             6  \n",
      "2            12  \n",
      "3             7  \n",
      "4            10  \n",
      "                                                text  label  sentiment  \\\n",
      "0    intuition or ego ? 3 simple step to reach truth      0     0.3400   \n",
      "1  physicist brings in particle from home he 's b...      1     0.0000   \n",
      "2  donald trump inspire new nsfw meaning of the a...      0     0.5719   \n",
      "3            trump style : insult and domestic abuse      0    -0.8176   \n",
      "4  man google 'tender lump on neck ' about to beg...      1     0.4767   \n",
      "\n",
      "   length_words  \n",
      "0            10  \n",
      "1            12  \n",
      "2            11  \n",
      "3             7  \n",
      "4            15  \n"
     ]
    }
   ],
   "source": [
    "dftrainSent['length_words'] = dftrainSent['text'].apply(lambda x: len(x.split()))\n",
    "dfvalidationSent['length_words'] = dfvalidationSent['text'].apply(lambda x: len(x.split()))\n",
    "dftestSent['length_words'] = dftestSent['text'].apply(lambda x: len(x.split()))\n",
    "print(dftrainSent.head())\n",
    "print(dfvalidationSent.head())\n",
    "print(dftestSent.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ef7130",
   "metadata": {},
   "source": [
    "## Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a31a67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "404f1d0a",
   "metadata": {},
   "source": [
    "## first/last word frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09185307",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05ed2593",
   "metadata": {},
   "source": [
    "## Bag of words (n-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39fd706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cfad2b11",
   "metadata": {},
   "source": [
    "## Parts of speech frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267bb35b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
